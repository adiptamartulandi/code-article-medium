{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "ke-RGiLhm-hJ"
      },
      "outputs": [],
      "source": [
        "#install dependencies\n",
        "!pip install -q langchain langchain-openai langchain_community langchain_chroma langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAAa3Iz4nRPp",
        "outputId": "08eb6b5a-e0f3-4ff0-d943-a28ab0deac8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create utility function to format the documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "RYdtiDSwnf2i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the document that we want to analyze\n",
        "# this document is a blog post from Microsoft Team related to LLMLingua2, tools to reduce token.\n",
        "loader = WebBaseLoader(web_paths=(\"https://aclanthology.org/2024.findings-acl.57/\",))\n",
        "docs = loader.load()\n",
        "\n",
        "# create a text splitter to split the document into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# convert into vector embeddings and save it in the vector store (Chroma)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "JgD4ii0qngTl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a retriever from the vector store\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "5cZO_es9v26T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#augment with prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LtxMisAywAJs",
        "outputId": "df6c1922-fbfc-43ed-9e3b-7ca692e42109"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate\n",
        "model = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# create rag-chain model\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"Who is the author of LLMLingua2 and what is the main idea of the paper?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KYW_GriLwGXl",
        "outputId": "c3352a56-fa1c-48a4-cb09-568a68e7fcb0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The authors of LLMLingua-2 are Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor RÃ¼hle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. The main idea of the paper is focused on data distillation for efficient and faithful task-agnostic prompt compression.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}